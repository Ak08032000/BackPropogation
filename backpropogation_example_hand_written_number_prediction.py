# -*- coding: utf-8 -*-
"""BackPropogation Example - Hand Written number prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R9y2le8DBO_pTdWsISAwmG6ypo0aycAe

#Example of BackPropogation.
Here we are going make a model which will predict the hand written numbers using backpropogation. First we will import all the libraries like tensorflow, keras, matplotlib. Then we will import the dataset mnist and devide all the inputs into training image, training lable, testing image and testing lable in which in the training part there will be 60000 set of images and for the testing part there will be 10000 set of images. Then will will normalize the images by dividing it by 255. Then we will start making the model in which there will be 1 input layer 4 hidden layers and 1 output layer. Then we will comple the model using the 'adam' optimizer and sparse_categorical_crossentropy loss function. 

#What is BackPropogation?
Backpropagation, short for "backward propagation of errors," is a widely used algorithm in machine learning for training artificial neural networks. It is used to calculate the gradient of the loss function with respect to the weights of the network, which is then used to update the weights in the direction that minimizes the loss function.

The backpropagation algorithm works by propagating the error from the output layer back to the input layer of the neural network. At each layer, the error is split and distributed to the neurons in the previous layer, proportional to the contribution of each neuron's output to the overall error. The derivative of the activation function is also taken into account to calculate the gradient of the error with respect to the weights.

Once the gradients are calculated, they are used to update the weights of the neural network through a process called gradient descent. The weights are adjusted in the opposite direction of the gradient, scaled by a learning rate hyperparameter, until the loss function is minimized.

Backpropagation is a powerful algorithm that has enabled the development of deep neural networks and has been instrumental in the success of many applications of machine learning, such as image recognition, natural language processing, and speech recognition.

#What is tensorflow?
TensorFlow is an open-source software library for numerical computation and machine learning, developed by the Google Brain team. It was first released in 2015 and has since become one of the most popular machine learning frameworks, widely used for developing and training machine learning models in a variety of fields, including computer vision, natural language processing, and robotics.

#What is keras?
Keras is an open-source high-level neural network API, written in Python and capable of running on top of TensorFlow, Theano, and other popular deep learning frameworks. Keras was developed to make it easier to build and train deep learning models by providing a user-friendly and intuitive interface.

#What is matplotlib?
Matplotlib is a popular open-source data visualization library for Python. It provides a wide range of tools for creating static, animated, and interactive visualizations in Python. Matplotlib was originally developed by John D. Hunter in 2003, and has since become one of the most widely used data visualization libraries in the Python ecosystem.
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

"""#Importing the dataset
Here we are going to import the dataset mnist and devide it in to training data and test data.

#What is mnist?
MNIST (Modified National Institute of Standards and Technology database) is a large dataset of handwritten digits that is commonly used in the field of machine learning for training and testing image recognition algorithms. The MNIST dataset consists of 60,000 training images and 10,000 test images, each of which is a grayscale image with a resolution of 28x28 pixels.

#What is load_data() do?
'load_ddata()'is a method in the Keras library that is used to load the MNIST dataset. The method returns two tuples representing the training and test sets, respectively.
"""

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

train_images.shape

test_images.shape

"""#Noramlizing dataset
Dividing image data of size 28x28 by 255 is a common technique used for normalizing pixel values to be in the range of 0 to 1. This technique is used to ensure that the data is scaled consistently across all images in the dataset, which can improve the performance of machine learning algorithms that rely on numerical stability and feature scaling.In the MNIST dataset, the pixel values of the images are represented as integers between 0 and 255, where 0 represents black and 255 represents white. By dividing the pixel values by 255, we rescale the pixel values to be in the range of 0 to 1, which makes it easier to work with the data and ensures that the pixel values are consistent across all images in the dataset.

#What is Normalization?
Normalization is a common technique used in machine learning to rescale input data to ensure that all features are on a similar scale. The goal of normalization is to make the data consistent and well-scaled, which can improve the performance of machine learning algorithms that rely on numerical stability and feature scaling.
"""

train_images, test_images = train_images/255, test_images/255 #Normalisation

train_images[0][5]

"""#Making layers of the model
First we will use sequantial which is a class in the Keras library that provides a way to create a linear stack of layers in a neural network. Then we will add layers in which the first layer or the input layer then there are all the hidden layer and at the end there is thevoutput layer. In this model all the hiden layers uses relu activation function and the outpu uses softmax activation function.

#What is Sequential()?
Sequential is a class in the Keras library that provides a way to create a linear stack of layers in a neural network. A neural network consists of multiple layers, each of which performs a specific computation on the input data. The Sequential class allows you to define the layers of a neural network in a simple and intuitive way, by specifying a sequence of layers that are applied to the input data in order.

#What is Flatten used for?
In Keras, flatten() is a method that can be called on a tensor or layer object, which is used to convert a multi-dimensional tensor into a one-dimensional array. The flatten() method is commonly used in deep learning models, particularly in the input layer, where the input data may have multiple dimensions, such as image data with height, width, and channels.

#What is Dense layers?
In Keras, Dense is a type of layer that represents a fully connected layer in a neural network. A fully connected layer is a type of layer in which every neuron in the layer is connected to every neuron in the previous layer. Each connection between neurons has a weight associated with it, which is learned during the training process.

#What is relu?
ReLU stands for Rectified Linear Unit, which is an activation function commonly used in deep learning models. The ReLU function is a simple but effective non-linear activation function that applies the function f(x) = max(0, x) to the output of a neuron. In other words, if the input to a ReLU neuron is positive, the neuron outputs the input value, but if the input is negative, the neuron outputs zero. This results in a simple thresholding operation that helps to introduce non-linearity into the neural network, allowing it to learn complex patterns in the data.

#What is softmax?
Softmax is an activation function that is commonly used in the output layer of neural networks for multi-class classification problems. The softmax function converts the output of a neural network into a probability distribution over the different possible classes. In other words, the softmax function exponentiates the inputs and then normalizes them by the sum of the exponentiated inputs. This ensures that the output probabilities sum to 1, and that each probability is between 0 and 1.
"""

model = models.Sequential()
model.add(layers.Flatten(input_shape=(28,28,1)))

model.add(Dense(50, activation='relu'))
model.add(Dense(40, activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(20, activation='relu'))

model.add(Dense(10, activation='softmax'))

model.summary()

"""#What is compile() do?
The compile() method is used to configure the learning process of a neural network model. When you call compile() on a Keras model, you specify several key parameters that control how the model will be trained:

optimizer: This specifies the optimization algorithm that will be used to update the weights of the model during training, such as stochastic gradient descent (SGD), Adam, or RMSprop.

loss: This specifies the loss function that will be used to measure the error of the model during training. The loss function is typically chosen based on the type of problem you are trying to solve, such as mean squared error (MSE) for regression problems or categorical cross-entropy for multi-class classification problems.

metrics: This specifies the evaluation metrics that will be used to monitor the performance of the model during training and testing. These metrics are typically chosen based on the type of problem you are trying to solve, such as accuracy for classification problems or mean squared error (MSE) for regression problems.
"""

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""#Training the network
The fit() method is used to train a neural network model on a dataset. When you call fit() on a Keras model, you specify the training data, validation data (optional), batch size, number of epochs, and other key parameters that control the training process.
"""

history = model.fit(train_images, train_labels, epochs=10,
          validation_data=(test_images, test_labels))

"""#Graphical representation of results"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')